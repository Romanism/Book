{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER4. 모델 훈련\n",
    "\n",
    "- 선형 회귀\n",
    "- 경사 하강법\n",
    "- 다항 회귀\n",
    "- 학습 곡선\n",
    "- 규제가 있는 선형모델\n",
    "- 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 선형 회귀 (Linear Regeression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{y} = h_\\theta (X) = \\theta^T * x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $h_\\theta (X)$는 모델 Parameter(모수) $\\theta$를 사용한 가설(hypothesis) 함수입니다.\n",
    "\n",
    "\n",
    "- $\\theta$는 편향 $\\theta_0$ 에서 $\\theta_n$까지 모델의 각 변수별 Parameter(모수)를 담은 벡터입니다.\n",
    "\n",
    "\n",
    "- $\\theta^T$는 $\\theta$의 전치입니다. (열 벡터가 아닌 행 백터)\n",
    "\n",
    "\n",
    "- $x$는 $x_0$ 에서 $x_n$까지 담고 있는 샘플의 특성 벡터입니다. $x_0$은 항상 1입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 성능 측정 지표 : 평균제곱오차 (Mean Square Error, MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE(X, h_\\theta) = \\dfrac{1}{m} \\sum_{i=1}^{m} (\\theta^T*x^{(i)} - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 회귀 모델에서 가장 많이 사용되는 성능 측정 지표는 RMSE입니다.\n",
    "\n",
    "\n",
    "- 하지만 실제로는 RMSE보다 MSE(평균제곱오차)를 최소화 하는것이 같은 결과를 내면서 더 간단합니다.\n",
    "\n",
    "\n",
    "- 간단한 이유는 어떤 함수를 최소화 하는 것은 그 함수의 제곱근을 최소화 하는것과 같기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 정규방정식 (Normal Equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형 회귀의 목표는 최적의 $\\theta$를 찾는 것입니다.\n",
    "\n",
    "\n",
    "- 최적의 $\\theta$를 찾기 위해선 비용함수 (Cost Fucntion)을 최소화해야 합니다.\n",
    "\n",
    "\n",
    "- 비용함수를 최소화하는 방법에는 크게 정규방정식 / 경사하강법을 이용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\theta} = (X^T * X)^{-1} * X^T * y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{\\theta}$은 비용함수를 최소화하는 $\\theta$값입니다.\n",
    "\n",
    "\n",
    "- $y$는 $y^{(1)}$ 부터 $y^{(m)}$ 까지 포함하는 타깃 벡터입니다.\n",
    "\n",
    "\n",
    "- 정규방정식의 학습된 선형 회귀 모델은 예측이 매우 빠릅니다. \n",
    "\n",
    "\n",
    "- 예측 계산 복잡도는 샘플 수와 특성 수에 선형적입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 경사하강법 (Gradient Descent, GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사하강법은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 매우 일반적인 최적화 알고리즘입니다.\n",
    "\n",
    "\n",
    "- 경사하강법의 기본 아이디어는 비용함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것입니다.\n",
    "\n",
    "\n",
    "- 볼록함수(x = $\\theta$, y = cost)에서 임의의 점을 찍고 그 함수의 꼭지점으로 점을 내려보냅니다.\n",
    "\n",
    "\n",
    "- 볼록함수의 꼭지점이 cost가 최소값이 되는데 이때의 $\\theta$가 최적의 파라미터입니다.\n",
    "\n",
    "\n",
    "- 경사하강법에서 중요한 파라미터는 스텝의 크기로 학습률(Learning rate) 하이퍼파라미터로 결정됩니다.\n",
    "\n",
    "\n",
    "- 학습률이 너무 작으면 꼭지점으로 가는데 너무 오래걸리고, 학습률이 너무 크면 꼭지점을 지나칠 수 있습니다.\n",
    "\n",
    "\n",
    "- 경사하강법은 특성이 매우 많고 훈련 샘플이 너무 많아 메모리에 담을 수 없을 때 적합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사하강법의 스텝 ( $\\eta$ = 학습률 (Learning rate) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\theta^{(next\\;step)} = \\theta - \\eta \\nabla_{\\theta}\\; MSE(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비용함수의 그래디언트 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\theta}\\; MSE(\\theta) = \\dfrac{2}{m} X^T*(X*\\theta - y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 배치 경사하강법 (Batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 매 경사하강법 스텝에서 전체훈련 세트 X에 대해 계산하는 방법입니다.\n",
    "\n",
    "\n",
    "- 안정성이 높은 장점이 있지만 시간이 오래걸리는 단점이 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 학률적 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 매 스텝에서 딱 한개의 샘플을 무작위로 선택하고 그 하나에 대한 그레디언트를 계산하는 방법입니다.\n",
    "\n",
    "\n",
    "- 매우 큰 훈련세트에서 학습이 가능하고 시간이 빠른 장점이 있지만 안정성이 낮은 단점이 있습니다.\n",
    "\n",
    "\n",
    "- 일반적으로 한 반복에서 m번 되풀이되고, 이때 각 반복을 에포크 (epoch)라고 합니다.\n",
    "\n",
    "\n",
    "- scikit-learn에서 SGD방식으로 선형회귀를 사용하면 기본 값으로 제곱 오차 비용함수를 최적화하는 SGDRegressor 클래스를 활용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 미니배치 경사하강법 (Mini Batch Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미니배치라 부르는 임의의 작은 샘플 세트에 대해 Gradient Descent를 계산하는 방법입니다.\n",
    "\n",
    "\n",
    "- GPU를 사용하면 성능이 향상됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|알고리즘|훈련샘플수가 클때|외부메모리 학습지원|특성수가 클때|하이퍼파라미터 수|스케일 조정 필요|사이킷 런|\n",
    "|--|--|--|--|--|--|--|\n",
    "|정규방정식|빠름|NO|느림|0|NO|LinearRegression|\n",
    "|배치 경사하강법|느림|NO|빠름|2|YES|n/a|\n",
    "|확률적 경사하강법|빠름|YES|빠름|>=2|YES|SGDRegressor|\n",
    "|미니배치 경사하강법|빠름|YES|빠름|>=2|YES|n/a|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 다항 회귀 (Polynomial Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가지고 있는 데이터가 직선이 아닌 복잡한 형태일 경우 사용하는 방법이 다항 회귀 입니다.\n",
    "\n",
    "\n",
    "- 제곱이나 세제곱 등을 추가시켜 비선형 데이터에도 적용 가능하도록 합니다.\n",
    "\n",
    "\n",
    "- 다항회귀를 통해 차수를 높이면 모델의 정확도는 높아지지만 Overfitting (과최적화)의 위험이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 학습 곡선 (Learning Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다항회귀는 Overffiting의 가능성을 높이는데 실제 모델이 얼마나 Oveffing이 됐는지 확인하는 방법입니다.\n",
    "\n",
    "\n",
    "- 학습 곡선은 Train Data와 Test Data의 모델 성능을 훈련 세트 크기의 함수로 나타냅니다.\n",
    "\n",
    "\n",
    "- Train Data에서 크가 다른 서브 Data를 만들어 모델을 여러 번 훈려시킵니다.\n",
    "\n",
    "\n",
    "- 과소적합 모델의 경우는 Train / Test 곡선이 수평한 구간을 만들고 꽤 높은 오차에서 매우 가까이 근접해 있습니다.\n",
    "\n",
    "\n",
    "- 과대적합 모델의 경우는 Train의 오차가 선형회귀모델보다 훨씬 낮고 두 곡선 사이에 공간이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * 편향 / 분산 트레이드 오프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 편향 (과소적합)\n",
    "\n",
    "- 일반화 오차 중에서 편향은 잘못된 가정으로 인한 것입니다.\n",
    "\n",
    "\n",
    "- 예를 들어 데이터가 실제로는 2차인데 선형으로 가정하는 경우입니다.\n",
    "\n",
    "\n",
    "- 편향이 큰 모델은 Train Data에 과소적합되기 쉽습니다.\n",
    "\n",
    "\n",
    "#### 2) 분산 (과대적합)\n",
    "\n",
    "- 훈련 데이터에 있는 작은 변동에 모델이 과도하게 민감하기 때문에 나타납니다.\n",
    "\n",
    "\n",
    "- 자유도가 높은 모델이 높은 분산을 가지기 쉬워 훈련 데이터에 과대적합되는 경향이 있습니다.\n",
    "\n",
    "\n",
    "#### 3) 줄일 수 없는 오차\n",
    "\n",
    "- 데이터 자체에 있는 노이즈 때문에 발생합니다.\n",
    "\n",
    "\n",
    "- 해결하는 유일한 방법은 데이터에서 노이즈를 제거하는 것입니다.\n",
    "\n",
    "\n",
    ": 모델이 복잡해지면 분산이 늘어나고 편향은 줄어듭니다. 반대로 모델이 단순해지면 분산이 감소하고 편향이 커집니다. 그래서 트레이드오프라 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 규제가 있는 선형 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과대적합을 막는 가장 효율적인 방법은 모델을 규제하는 것입니다.\n",
    "\n",
    "\n",
    "- 일반적으로 회귀모델을 규제하기 위해서 가중치를 제한하는 방법이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 릿지 (Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치들의 제곱합을 최소화하는 것을 추가적인 제약조건으로 하는 방법입니다.\n",
    "\n",
    "\n",
    "- 기본적으로 사용하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = MSE(\\theta) + \\alpha \\dfrac{1}{2}\\sum_{i=1}^{n}\\theta_i ^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\alpha$를 통해 모델을 얼마나 규제할지 조정할 수 있습니다.\n",
    "\n",
    "\n",
    "- $\\alpha$가 크면 규제가 커지고 $\\alpha$가 작으면 규제가 작아집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 라쏘 (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치의 절대값의 합을 추가적인 제약조건으로 하는 방법입니다.\n",
    "\n",
    "\n",
    "- 실제 쓰이는 특성의 개수가 얼마 없을 때 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = MSE(\\theta) + \\alpha \\dfrac{1}{2}\\sum_{i=1}^{n}|\\theta_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 규제의 의미는 덜 중요한 특성의 가중치를 완전히 제거하는 것입니다.\n",
    "\n",
    "\n",
    "- 자동으로 특성을 선택하고 희소모델을 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 엘라스 넷 (Elastic Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 릿지와 라쏘를 결합한 형태입니다.\n",
    "\n",
    "\n",
    "- 특성 수가 훈련샘플 수보다 크거나 특성 몇개가 강하게 연관되어 있을 때 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = MSE(\\theta) + r*\\alpha \\dfrac{1}{2}\\sum_{i=1}^{n}|\\theta_i|+ \\dfrac{(1-r)}{2}*\\alpha \\dfrac{1}{2}\\sum_{i=1}^{n}\\theta_i ^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $r = 1$ 이면 라쏘, $r = 0$이면 릿지를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4 조기 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에러가 최솟값에 도달하면 바로 훈련을 중지하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 로지스틱 회귀 (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 샘플이 특정 클래스에 속할 확률을 추정하는데 사용합니다.\n",
    "\n",
    "\n",
    "- 확률이 50% 이상이면 속하고 50%보다 작으면 속하지 않다고 판단합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.1 로지스틱 회귀모델의 확률추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{p} = h_\\theta(x) = \\sigma(\\theta^T * x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로지스틱 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(t) = \\dfrac{1}{1+exp(-t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로지스틱 함수는 0과 1사이의 값을 출력하는 시그모이드 함수입니다. (S자 형태)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2 훈련과 비용 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = -\\dfrac{1}{m}\\sum_{i=1}^{m}\\;[y^{(i)}\\;log(\\hat{p}^{(i)}) + (1-y^{(i)})\\;log(1-\\hat{p}^{(i)})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3 소프트맥스 회귀 (Softmax Regerssion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여러 개의 이진분류문제를 훈련시켜 연결하지 않고 직접 다중 클래스를 지원하도록 해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{p_k} = \\sigma(S(X))_k = \\dfrac{exp(S_k (X))}{\\sum_{j=1}^{k} exp(s_j (X))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k 는 클래스 수를 의미합니다.\n",
    "\n",
    "\n",
    "- $\\sigma(S(X))_k$는 클래스에 속할 확률을 의미합니다.\n",
    "\n",
    "\n",
    "- $s_j (X)$는 클래스의 점수를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = argmax\\;\\sigma(S(X))_k = argmax\\;S_k(X) = argmax\\;((\\theta^{(k)})^T X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 클래스의 확률을 비교해 가장 큰 확률의 클래스를 선택합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.4 크로스엔트로피 비용함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta) = -\\dfrac{1}{m} \\sum_{i = 1}^{m}\\sum_{j = 1}^{k} y^{(i)}_k log(\\hat{p_k}^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크로스엔트로피 Gradient Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_{\\theta^{(k)}}\\;J(\\theta) = \\dfrac{1}{m}\\sum_{i=1}^{m} (\\hat{p_k}^{(i)} - y^{(i)}_k)\\;X^{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
